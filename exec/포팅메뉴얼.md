# 이것만 따라하면 당신도 쿠버네티스 마스터

<img src="https://s-media-cache-ak0.pinimg.com/564x/15/00/15/150015b37ecf708c0eba6c82b1a5075b.jpg" title="" alt="당신도가능" data-align="center">

***

# 포팅 메뉴얼

------------------------------------------------

## 개발환경

--------------------------------------

- java 17

- spring boot 3.1.3

- Nginx 1.18.0

- postgresql 15.3

- jenkins 2.414.3

- next.js 18.16.1

- kubernates 1.28.2

## 포트번호

---------------------------------------

- jenkins
  
  - ```8081:8080```

- SpringBoot
  
  - user
    
    - ```8101:8101```
  
  - troubleshooting
    
    - ```8102:8102```

- Nginx
  
  - ```80:80```

- RabbitMQ
  
  - ```5672:5672```
  
  - ```15672:15672```

- next.js
  
  - ```3000:3000```

- redis
  
  - ```7777:6379```

- postgresql
  
  - ```5432:5432```

- kubernetes
  
  - API server
    
    - ```6443:6443```
  
  - Kubelet API
    
    - ```10250:10250```
  
  - NodePort Services
    
    - ```30000 ~ 32767```

## 배포 가이드

-------------------------------

### 브랜치 전략

- ```master``` : 제출용

- ```fe/*- bakc/*``` : 실제 배포되는 서버

### 도커 설치

```
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt update
sudo apt install docker-ce
```

### postgreSQL 설치 (도커)

```
docker run --name postgres_container -e POSTGRES_PASSWORD=wjdtmfgh -p 5432:5432 -d postgres:alpine3.18
```

### Redis 설치 (도커)

```
docker pull redis
docker run -p 7777:6379 --name redis_container -d redis:latest --requirepass dlfgofkwjdtmfgh205
```

### RabbitMQ 설치 (도커)

```
docker run -itd --name rabbitmq -p 5672:5672 -p 15672:15672 --restart=unless-stopped -e RABBITMQ_DEFAULT_USER=wjdtmfgh -e RABBITMQ_DEFAULT_PASS=rlatngus rabbitmq:management
```

### Nginx 설치

```
sudo apt update
sudo apt install ngin

sudo systemctl status nginx
sudo systemctl start nginx
sudo systemctl restart nginx
```

#### Nginx.conf

```
 server {
                listen 443 ssl;
                server_name orientalsalad.kro.kr;

                ssl_certificate /etc/letsencrypt/live/orientalsalad.kro.kr/fullchain.pem;
                ssl_certificate_key /etc/letsencrypt/live/orientalsalad.kro.kr/privkey.pem;

                location / {
                        proxy_pass http://43.201.102.112:31549/;
                        proxy_set_header X-Real-IP $remote_addr;
                        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                        proxy_set_header Host $http_host;
                }
        }

        server {
                listen 80;
                server_name orientalsalad.kro.kr;
                location / {
                        return 301 https://$host$request_uri;
                }
        }
```

#### Nginx ssl 발급

```
https://minsigi.tistory.com/9 -> standalone
```

### Jenkins 설치

```
docker run -itd --name jenkins -p 8081:8080  -v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker  -e TZ=Asia/Seoul -u root jenkins/jenkins:lts-jdk17
```

#### Jenkins 패스워드

```
sudo cat /var/jenkins_home/secrets/initialAdminPassword

못찾겠으면
sudo find / -name "initialAdminPassword

컨테이너 안에 있을경우
docker exec [컨테이너 ID 또는 이름] cat /var/jenkins_home/secrets/initialAdminPassword"
```

#### WebHook

1. Jenkins 접속 및 로그인

2. Jenkins 패스워드 입력

3. install suggested plugins을 클릭해 기본 플러그인 설치

4. Dashboard → jenkins관리 → Plugins→ Available plugins 에서 gitlab설치

5. Dashboard → jenkins관리 → Credentials → gitlab_api, gitlab 계정 연동

6. Dashboard → 새로운 item → pipeline 생성

7. gitlab Connection 설정 → Use alternative credential → Credential 입력

8. Build when a change is pushed to GitLab. GitLab webhook URL: http://43.200.130.229:8081/project/df 체크 → 고급에서 토큰 발급

9. Pipeline script form SCM 설정 → SCM에서 git 설정 → Script Path에 Jenkinsfile 위치 입력 (fe/Jenkinsfile), (back/user/jenkinsfile)

10. gitlab에서 webhook, 토큰 발급 받아서 Jenkins와 연동

11. root 디렉토리에 Jenkinsfile, dockerfile 필요 (dockerhub를 이용해서 배포)

#### jenkinsfile(front)

```
pipeline {
    agent any
    environment {
        DOCKER = 'sudo docker'
    }

    stages {
        stage('Check directory') {
            steps {
                sh 'ls -al'
                script {
                    def commitAuthor = sh(script: 'git log -1 --pretty=%an', returnStdout: true).trim()
                    env.COMMIT_AUTHOR = commitAuthor

                    def commitMessage = sh(script: 'git log -1 --pretty=%B', returnStdout: true).trim()
                    env.COMMIT_MESSAGE = commitMessage
                }
            }
        }

        stage('Clone Repository') {
            steps {
                mattermostSend color: 'warning', message: ":angry_jenkins_face: 배포 시작! :angry_jenkins_face: \n`front - #${env.BUILD_NUMBER}`\nStarted by GitLab push by ${env.COMMIT_AUTHOR}\n링크: ${env.BUILD_URL}"
                checkout scm
                echo 'Checkout Scm'
            }
        }

        stage('Remove Old Docker Image') {
            steps {
                script {
                    try {
                        sh 'docker rmi godseye93/front:latest || true'
                    } catch (e) {
                        echo 'Failed to remove old Docker image'
                    }
                }
            }
        }

        stage('Build image') {
            steps {
                dir('FE') {
                    sh 'ls -al'
                    sh 'docker login -u {아이 -p {비밀번호}'
                    sh 'docker build -t godseye93/front:latest .'
                }
                echo 'Build image...'
            }
        }

        stage('Push image') {
            steps {
                sh 'docker push godseye93/front:latest'
                echo 'Push image to DockerHub...'
            }
        }

        stage('Remove Previous image') {
            steps {
                script {
                    try {
                        sh 'docker stop front'
                        sh 'docker rm front'
                    } catch (e) {
                        echo 'fail to stop and remove container'
                    }
                }
            }
        }

        stage('Run New image') {
            steps {
                sh 'docker run --name front -d -p 3000:3000 godseye93/front:latest'
                echo 'Run New member image'
            }
        }
    }
    post {
        success {
            mattermostSend color: 'good', message: ":jampepebyyeah: 배포 완료! :jampepebyyeah: \n`front - #${env.BUILD_NUMBER}`\nStarted by changes from ${env.COMMIT_AUTHOR}\n**${env.COMMIT_MESSAGE}** \n링크: ${env.BUILD_URL}"
        }
        failure {
            mattermostSend color: 'danger', message: ":angry_pepe_2: 배포 실패! :angry_pepe_2: \n:angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2: \n:angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2:"
        }
    }

}
```

#### dockerfile(front)

```
# Node.js 18 버전을 기반 이미지로 사용
FROM node:18-alpine

# 앱 디렉터리 생성
WORKDIR /user/src/app

# 앱 의존성 설치
COPY package*.json ./
RUN npm install

# 앱 소스 추가
COPY . .

# 앱 빌드
RUN npm run build

# 서버가 3000번 포트에서 실행되므로 Docker에서도 이 포트를 열어준다는뜻
EXPOSE 3000

# 앱 실행 명령어
CMD [ "npm", "run", "start" ]
```

#### Jenkinsfile(user)

```
pipeline {
    agent any
    environment {
        DOCKER = 'sudo docker'
    }

    stages {
        stage('Check directory') {
            steps {
                sh 'ls -al'
                script {
                    def commitAuthor = sh(script: 'git log -1 --pretty=%an', returnStdout: true).trim()
                    env.COMMIT_AUTHOR = commitAuthor

                    def commitMessage = sh(script: 'git log -1 --pretty=%B', returnStdout: true).trim()
                    env.COMMIT_MESSAGE = commitMessage
                }
            }
        }


        stage('Clone Repository') {
            steps {
                mattermostSend color: 'warning', message: ":angry_jenkins_face: 배포 시작! :angry_jenkins_face: \n`back/user - #${env.BUILD_NUMBER}`\nStarted by GitLab push by ${env.COMMIT_AUTHOR}\n링크: ${env.BUILD_URL}"
                checkout scm
                echo 'Checkout Scm'
            }
        }

        stage('Remove Old Docker Image') {
            steps {
                script {
                    try {
                        sh 'docker rmi godseye93/user:latest || true'
                    } catch (e) {
                        echo 'Failed to remove old Docker image'
                    }
                }
            }
        }

        stage('Build image') {
            steps {
                dir('user') {
                    sh 'ls -al'
                    sh 'chmod +x ./gradlew'
                    sh './gradlew build'
                    sh 'ls -al build/libs/'
                    sh 'docker login -u {아이} -p {비밀번호}'
                    sh 'docker build -t godseye93/user:latest .'
                }
                echo 'Build image...'
            }
        }

        stage('Push image') {
            steps {
                sh 'docker push godseye93/user:latest'
                echo 'Push image to DockerHub...'
            }
        }

        stage('Remove Previous image') {
            steps {
                script {
                    try {
                        sh 'docker stop user'
                        sh 'docker rm user'
                    } catch (e) {
                        echo 'fail to stop and remove container'
                    }
                }
            }
        }
        stage('Run New image') {
            steps {
                sh 'docker run --name user -d -p 8101:8101 godseye93/user:latest'
                echo 'Run New member image'
            }
        }
    }
    post {
        success {
            mattermostSend color: 'good', message: ":jampepebyyeah: 배포 완료! :jampepebyyeah: \n`back/user - #${env.BUILD_NUMBER}`\nStarted by changes from ${env.COMMIT_AUTHOR}\n**${env.COMMIT_MESSAGE}** \n링크: ${env.BUILD_URL}"
        }
        failure {
            mattermostSend color: 'danger', message: ":angry_pepe_2: 배포 실패! :angry_pepe_2: \n:angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2: \n:angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2: :angry_pepe_2:"
        }
    }
}
```

#### dockerfile(user)

```
FROM openjdk:17
COPY build/libs/troubleShot-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT ["java", "-jar", "/app.jar","--server.port=8101"]
```

# Kubernetes

준비물 : 3개의 서버 (1개의 master node, 2개의 worker node) - 같은 VPC 권장

#### 1. 설치전 환경 설정

1.1 Swap Disable

```
sudo swapoff -a && sudo sed -i '/swap/s/^/#/' /etc/fstab


// 설정확인
// Swap 에 할당이 0이면 된다.
free -h
```

1.2 iptable 설정 ( ubuntu 배포판에선 이미 설정되어 있다)

```
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

// 설정확인
sudo sysctl -p /etc/sysctl.d/k8s.conf
```

1.3 방화벽 해제

```
sudo systemctl stop firewalld
sudo systemctl disable firewalld
```

1.4 컨테이너 런타임 설치

```
런타임                                 유닉스 도메인 소켓 경로
containerd                             unix:///var/run/containerd/containerd.sock
CRI-O                                   unix:///var/run/crio/crio.sock
도커 엔진 (cri-dockerd 사용)           unix:///var/run/cri-dockerd.sock
```

*docker 설치시 containerD도 같이 설치 된다*

```
sudo systemctl status containerd
containerd --version
```

#### 2. Kubeadm, Kubectl, Kubelet 설치 (버전 고정)

2.1 kubeadm, kubectl, kubelet 설치

```
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubeadm kubectl kubelet
sudo apt-mark hold kubelet kubeadm kubectl
```

2.2 gpg 오류

```
ubuntu@ip-172-31-17-194:~$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
OK
ubuntu@ip-172-31-17-194:~$ sudo apt-get update
Hit:1 http://ap-northeast-2.ec2.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://ap-northeast-2.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://ap-northeast-2.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 https://download.docker.com/linux/ubuntu focal InRelease
Hit:6 http://security.ubuntu.com/ubuntu focal-security InRelease
Get:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [8993 B]
Err:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY B53DC80D13EDEF05
Reading package lists... Done
W: GPG error: https://packages.cloud.google.com/apt kubernetes-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY B53DC80D13EDEF05
E: The repository 'https://apt.kubernetes.io kubernetes-xenial InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.

// 가 뜬다면
// curl 명령을 사용하여 GPG 키를 가져올 때 파일을 현재 사용자의 홈 디렉토리에 저장하려는 것이 아니라 /tmp 디렉토리에 저장
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /tmp/kubernetes-archive-keyring.gpg

// /usr/share/keyrings/ 디렉토리에 복사
sudo cp /tmp/kubernetes-archive-keyring.gpg /usr/share/keyrings/kubernetes-archive-keyring.gpg

// 다시 업데이트
sudo apt-get update
```

2.3 서비스 등록 및 재설치

```
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

#### 3. control - plane 구성 (마스터 노드)

3.1 master에서만 실행 ( worker node에서 하면 대참사남)

```
kubeadm init
```

3.2 container runtime is not running: output 오류

```
ubuntu@ip-172-31-17-194:~$ sudo kubeadm init
I1106 04:10:39.261599   26722 version.go:256] remote version is much newer: v1.28.3; falling back to: stable-1.26
[init] Using Kubernetes version: v1.26.10
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2023-11-06T04:10:39Z" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher


//해결법
sudo rm /etc/containerd/config.toml
sudo systemctl restart containerd
sudo kubeadm init
```

3.3 초기화(설정에 오류가 생겼을때)

```
sudo kubeadm reset
```

3.4 worker node에 join하기 위한 토큰 값 저장

```
cat > token.txt
contrl + d
cat token.txt


// 하거나 메모장에 저장하거
```

3.5 weave.net 설치

```
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
```

3.6 config view

- sudo kubeadm init 이후

- kubectl은 기본적으로 $HOME/.kube/config를 참조한다

- 하지만 init 이후, 해당 위치에 config가 없기 때문에

```
// $HOME/.kube 디렉토리 생성
mkdir -p $HOME/.kube

// /etc/kubernetes/admin.conf 파일을 $HOME/.kube/config로 복사
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

// $HOME/.kube/config를 파일 소유자를 현재 사용자의 UID와 GID로 변경
udo chown $(id -u):$(id -g) $HOME/.kube/config

// 이후 를 입력하여 실제로 참조하는 위치를 확인하는데
echo $KUBECONFIG

1. /etc/kubernetes/admin.conf 이경우는 경로를 제대로 설정되지 않았기에
-> export KUBECONFIG=~/.kube/config  를 입력하여 경로를 변경

2. /home/ubuntu/.kube/config 이경우는 경로를 제대로 읽고 있다

마지막으로

kubectl config view 를 입력하여

제대로 연결된거 확인
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.26.8.252:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: DATA+OMITTED
    client-key-data: DATA+OMITTED
```

3.7 Cgroup Driver 확인 -> 

```
docker info | grep Cgroup -F2

vi /usr/lib/systemd/system/docker.service

#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd
로 수정

systemctl daemon-reload
systemctl restart docker

다시 docker info | grep Cgroup -F2 로 정상적으로 바뀌었는지 확인
```

- kubectl config view
  
  - The connection to the server localhost:8080 was refused - did you specify the right host or port? 오류 발생 -> 3.6 참고

```
ubuntu@ip-172-26-8-252:~$ sudo kubectl config view
apiVersion: v1
clusters: null
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null
```

#### 4. worker node 구성하기

- 토큰 그대로 복사 붙여넣기

4.1 설치확인

```
kubectl get nodes
kubectl get nodes -o wide
kubectl get pod --all-namespaces
```

4.2 bash 환경에서 kubectl 명령어 자동 완성

```
sudo apt install bash-completion
source <(kubectl completion bash)
echo 'source <(kubectl completion bash)' >> ~/.bashrc
```

4.3 worker node가 NotReady에서 변경되지 않는경우

```
ubuntu@ip-172-31-0-91:~$ kubectl get nodes
NAME               STATUS     ROLES           AGE   VERSION
ip-172-31-0-91     Ready      control-plane   75m   v1.28.2
ip-172-31-16-228   NotReady   <none>          74m   v1.28.2
ip-172-31-4-103    Ready      <none>          74m   v1.28.2
```

- masternode에서 문제가 되는 node를 체크한다

- ```
  kubectl describe node ip-172-31-16-228
  //여기서 Conditions을 확인하면
  
  ```
  Ready                False   Fri, 10 Nov 2023 06:10:10 +0000   Fri, 10 Nov 2023 06:04:54 +0000   KubeletNotReady              
  container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
  ```
  // 이런 오류를 볼수있는데 대부분 NetworkPlugin 오류 즉, CNI (weave net, Calico, Flannel) 문제지만
  // 나의 경우는 다른 worker ndoe는 정상 작동하고 kubeadm reset 통하여 CNI를 재설치해봐도 같은 증상이었다
  ```

- 문제가 되는 node에 접속하여 kubelet 로그를 확인

- ```
  journalctl -u kubelet -n 100
  
  ip-172-31-16-228 kubelet[456530]: E1110 07:21:42.131619  456530 remote_runtime.go:222] 
  "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = fa
  
  // 이런 오류내용이 있었었는데
  // StopPodSandbox는 container runtime 관련된 호출인걸 확인
  containerD 가 제대로 설치된걸 확인후
  
  systemctl status containerd
  // containerd 상태 확인
  // 마찬가지로 sandbox 관련된 오류코드 발견
  
  
  systemctl restart containerd
  // containerd 재시작
  ```

- 다시 마스터 노드로 가서 kubectl get nodes 입력후 정상적으로 Ready 된것 확인
  
  

#### 5. 쿠버네티스 클러스터 구성하기



5.1 Master Node에서 pods 배포하기

- user, front, troubleshooting 3종류의 기능, 3개의 레플리카(복제품) 생성

- yaml을 만들어서 아래의 명령어로 적용

- ```
  kubectl apply -f [yaml 이름]
  ```

- Python 등 과 같이 띄어쓰기로 구분하고 teb은 사용금지

5.1.1 user.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user
  template:
    metadata:
      labels:
        app: user
    spec:
      containers:
      - name: user
        image: godseye93/user:latest
        ports:
        - containerPort: 8101
      imagePullSecrets:
      - name: dockerhub-credentials
```

5.1.2 front.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: front-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: front
        image: godseye93/front:latest
        ports:
        - containerPort: 3000
      imagePullSecrets:
      - name: dockerhub-credentials
```

5.1.3 troubleshooting.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: troubleshooting-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: troubleshooting
  template:
    metadata:
      labels:
        app: troubleshooting
    spec:
      containers:
      - name: troubleshooting
        image: godseye93/troubleshooting:latest
        ports:
        - containerPort: 8102
      imagePullSecrets:
      - name: dockerhub-credentials
~                                      
```



5.2 service.yaml 작성

5.2.1 front-service.yaml (deployment, service 모두 apply 해야지 정상작동한다)

```
apiVersion: v1
kind: Service
metadata:
  name: front-service
spec:
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
  type: ClusterIP
```

5.2.2 user-service.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: user-service
spec:
  selector:
    app: user
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8101
  type: ClusterIP                                                 
```

5.2.3 troubleshooting-service.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: troubleshooting-service
spec:
  selector:
    app: troubleshooting
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8102
  type: ClusterIP
```



5.3 nginx-ingress-controller 설치

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v<버전>/deploy/static/provider/cloud/deploy.yaml
```

- nginx-ingress-controller는 내부네트워크로 통신하는 쿠버네티스 클러스트를

- 외부와 연결해주고 각각의 service로 라우팅해주는 기능을 하고있다

- 외부랑 연결하는 방법은 크게3가지가 있고 2가지를 추천한다
  
  1. AWS 클라우드를 이용한다면 ELB를 이용하여 자동으로 자체로드밸런서와 nginx-ingress-controller을 연결시켜준다
     
     장점 : AWS단에서 로드밸런싱도 해주고 관리도 쉽고 편하고 좋은점 천지다
     
     단점 : 해당 서비스는 EKS에서 쉽게 적용되므로 추가적인 비용이 발생한다

           2. nodeport 방식으로 모든 워커노드의 동일한 포트를 1개 열어서 해당 포트로 외부랑 

                연결한다

                장점 : 특별한 비용없고 쉽고 간단하게 클러스터 내부랑 연결이 가능하다

                단점 : 포트번호가 외부에 노출된다 -> 우리같은경우는 외부서버에 nginx를 설치해서 nginx-ingress-controller의 리다이렉트하고 포트번호를 숨겼다



5.3.1 ingress.yaml 생성후 적용

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: front-service
            port:
              number: 80
  - http:
      paths:
      - pathType: ImplementationSpecific
        path: "/user/(.*)"
        backend:
          service:
            name: user-service
            port:
              number: 80
  - http:
      paths:
      - pathType: ImplementationSpecific
        path: "/troubleshooting/(.*)"
        backend:
          service:
            name: troubleshooting-service
            port:
              number: 80
```

- nodeport를 이용하여 외부접속을 허용한뒤 받은 외부접속을 service로 라우팅해주는 용도이다
